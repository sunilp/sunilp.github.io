<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Sunil Prakash</title>
    <link>http://sunilprakash.com/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on Sunil Prakash</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Sunil Prakash</copyright>
    <lastBuildDate>Wed, 27 Sep 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/machine-learning/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Human Activity Recognition</title>
      <link>http://sunilprakash.com/project/human-activity-recognition/</link>
      <pubDate>Wed, 27 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>http://sunilprakash.com/project/human-activity-recognition/</guid>
      <description>

&lt;h3 id=&#34;problem-description&#34;&gt;Problem   Description&lt;/h3&gt;

&lt;p&gt;The human activity detection has many applications in several fields like biometrics, surveillance, help monitor aged people in home environments etc . Different types of sensors can be used to address this task. The use of multi modal sensors for human activity recognition is increasing these days. Data from four different temporarily synchronised modalities is available. Using this data of either all the types or any single type the human activities   must   be   classified.&lt;/p&gt;

&lt;h3 id=&#34;data-set-explanation&#34;&gt;Data   Set   Explanation&lt;/h3&gt;

&lt;p&gt;The dataset is a freely available dataset named UTD-MHAD, that has four temporarily synchronized data modalities. These modalities include RGB videos, depth videos, skeleton positions, and inertial signals from a Kinect camera and a wearable inertial sensor for a comprehensive set of 27 human actions performed by 8 subjects (4 males, 4 females). Each subject repeated each action 4 times. After removing three corrupted sequences, the dataset   includes   861   data   sequences.&lt;/p&gt;

&lt;h3 id=&#34;technological-approach&#34;&gt;TECHNOLOGICAL   APPROACH&lt;/h3&gt;

&lt;p&gt;The approach employed is the Hidden Markov Model (HMM), which works very well with sequential data and is essentially a Markov process with hidden and unobservable states. The actions (or output) are visible to the viewer, but the sequences that lead to the output are hidden. The model was trained on the skeleton, inertial and depth data for the first 3 actions   -   swipe   left,   swipe   right   and   wave.&lt;/p&gt;

&lt;h3 id=&#34;experimental-results-performance-evaluation&#34;&gt;EXPERIMENTAL   RESULTS   &amp;amp;   PERFORMANCE   EVALUATION&lt;/h3&gt;

&lt;p&gt;The results of the testing were favourable in general. The skeleton model achieved an accuracy of 67%, inertial achieved 94% and depth, 67%. The inertial model was the most accurate due to the nature of the data since the actions were only recorded in a few
   3
seconds and there was insignificant data loss, the model would recognize the actions very well.
As for the depth model, it depends on visual details and requires some effort to extract features based on edge detection (performed with the Sobel-Feldman algorithm). Initially the features were extracted and convoluted(CNN) before applying the HMM classification.Since the depth sequences may be sensitive to occlusions and the textures of the images are not as good as that of coloured images,the model turned out to be inaccurate. After applying the edge detection algorithm and the corresponding feature extraction,the accuracy improved considerably to 67%.The results could possibly be improved   with   a   larger   dataset   acquired   through   additional   sensors.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sign Language Recognition</title>
      <link>http://sunilprakash.com/project/sign-language-recognition/</link>
      <pubDate>Tue, 13 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>http://sunilprakash.com/project/sign-language-recognition/</guid>
      <description>

&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;As part of Udacity AIND, it was required to build a system that can recognize words communicated using the American Sign Language (ASL). the preprocessed dataset of tracked hand and nose positions extracted from video was provided.  Goal is be to train a set of Hidden Markov Models (HMMs) using part of this dataset to try and identify individual words from test sequences.&lt;/p&gt;

&lt;p&gt;Also incorporating Statistical Language Models (SLMs) that capture the conditional probability of particular sequences of words occurring. This improves the recognition accuracy of the system.&lt;/p&gt;

&lt;h3 id=&#34;implementation&#34;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/sunilp/sign-language-recognizer/blob/master/asl_recognizer.ipynb&#34;&gt;https://github.com/sunilp/sign-language-recognizer/blob/master/asl_recognizer.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
